{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpVXKAHtXVn5VoBw4i9RVN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KayalvizhiT513/CBOW-Algorithm-/blob/main/CBOW_1_context.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Correct one\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "class CBOW:\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, learning_rate=0.001):\n",
        "        np.random.seed(42)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.W1 = np.array([[-0.71984421, -0.46063877,  1.05712223,  0.34361829, -1.76304016],\n",
        "                            [ 0.32408397, -0.38508228, -0.676922  ,  0.61167629,  1.03099952],\n",
        "                            [ 0.93128012, -0.83921752, -0.30921238,  0.33126343,  0.97554513]])\n",
        "        self.W2 = np.array([[-0.47917424, -0.18565898, -1.10633497],\n",
        "                            [-1.19620662,  0.81252582,  1.35624003],\n",
        "                            [-0.07201012,  1.0035329 ,  0.36163603],\n",
        "                            [-0.64511975,  0.36139561,  1.53803657],\n",
        "                            [-0.03582604,  1.56464366, -2.6197451 ]])\n",
        "\n",
        "    def generate_one_hot(self, word_idx):\n",
        "        one_hot = np.zeros(self.vocab_size)\n",
        "        one_hot[word_idx] = 1\n",
        "        return one_hot\n",
        "\n",
        "    def linear_activation(self, x):\n",
        "        return x\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x)  #exp_x = np.exp(x - np.max(x))\n",
        "        return exp_x / exp_x.sum(axis=0)\n",
        "\n",
        "    def forward_pass(self, context):\n",
        "        hidden_layer_input = np.dot(self.W1, context)\n",
        "        hidden_layer_activation = self.linear_activation(hidden_layer_input)\n",
        "        output_layer = np.dot(self.W2, hidden_layer_activation)\n",
        "        output_probs = self.softmax(output_layer)\n",
        "        return hidden_layer_activation, output_probs\n",
        "\n",
        "    def backward_pass(self, context, target, hidden_layer_activation, output_probs):\n",
        "        error_output = output_probs - target\n",
        "        dW2 = np.outer(error_output, hidden_layer_activation)\n",
        "        error_hidden = np.dot(self.W2.T, error_output)\n",
        "        dW1 = np.outer(error_hidden, context)\n",
        "        return dW1, dW2\n",
        "\n",
        "    def update_weights(self, dW1, dW2):\n",
        "        self.W1 -= self.learning_rate * dW1\n",
        "        self.W2 -= self.learning_rate * dW2\n",
        "\n",
        "    def train(self, corpus, vocabulary = None, epochs=1000):\n",
        "        epoch_vs_loss = {}\n",
        "        for term in vocabulary:\n",
        "            epoch_vs_loss[term] = {}\n",
        "        for epoch in range(epochs):\n",
        "            for sentence in corpus:\n",
        "                context_words = sentence.split()[:-1]\n",
        "                target_word = sentence.split()[-1]\n",
        "\n",
        "                context_vectors = [self.generate_one_hot(self.word_index[word]) for word in context_words]\n",
        "                context = np.mean(context_vectors, axis=0)\n",
        "                target = self.generate_one_hot(self.word_index[target_word])\n",
        "\n",
        "                hidden_layer_activation, output_probs = self.forward_pass(context)\n",
        "                epoch_vs_loss[target_word][epoch] = output_probs\n",
        "                dW1, dW2 = self.backward_pass(context, target, hidden_layer_activation, output_probs)\n",
        "                self.update_weights(dW1, dW2)\n",
        "        return epoch_vs_loss\n",
        "\n",
        "    def predict(self, input_word):\n",
        "        if input_word in self.word_index:\n",
        "            input_vector = self.generate_one_hot(self.word_index[input_word])\n",
        "            _, output_probs = self.forward_pass(input_vector)\n",
        "            predicted_word_index = np.argmax(output_probs)\n",
        "            return self.index_word[predicted_word_index], output_probs\n",
        "        else:\n",
        "            return \"Word not in vocabulary\", None"
      ],
      "metadata": {
        "id": "5dE27s9925T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "corpus = [\n",
        "    \"eat apple\",\n",
        "    \"eat banana\",\n",
        "    \"eat sapota\",\n",
        "    \"eat carrot\"\n",
        "]\n",
        "\n",
        "word_count = defaultdict(int)\n",
        "for sentence in corpus:\n",
        "    for word in sentence.split():\n",
        "        word_count[word] += 1\n",
        "\n",
        "sorted_words = sorted(word_count, key=word_count.get, reverse=True)\n",
        "word_index = {word: i for i, word in enumerate(sorted_words)}\n",
        "index_word = {i: word for word, i in word_index.items()}\n",
        "vocab_size = len(sorted_words)\n",
        "embedding_dim = vocab_size  # Set embedding_dim to match vocab_size\n",
        "hidden_dim = 3  # Set the desired number of hidden dimensions\n",
        "\n",
        "cbow = CBOW(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim)\n",
        "cbow.word_index = word_index\n",
        "cbow.index_word = index_word\n",
        "epoch_vs_loss = cbow.train(corpus, sorted_words, epochs=10000)\n",
        "\n",
        "# Convert weight matrices to pandas DataFrames\n",
        "W1_df = pd.DataFrame(cbow.W1, index=[f'Hidden_dim_{i}' for i in range(cbow.W1.shape[0])], columns=sorted_words)\n",
        "W2_df = pd.DataFrame(cbow.W2, index=sorted_words, columns=[f'Hidden_dim_{i}' for i in range(cbow.W2.shape[1])])\n",
        "\n",
        "print(\"The corpus: \\n\",corpus)\n",
        "print(\"Final Input weight matrix of the hidden layer (W^T . x):\")\n",
        "print(W1_df)\n",
        "print(\"\\nFinal Output weight matrix of the hidden layer (W^T . h):\")\n",
        "print(W2_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_cTuXA_EWn1",
        "outputId": "f4436f22-4927-48e9-9c16-7637f18ed821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The corpus: \n",
            " ['eat apple', 'eat banana', 'eat sapota', 'eat carrot']\n",
            "Final Input weight matrix of the hidden layer (W^T . x):\n",
            "                   eat     apple    banana    sapota    carrot\n",
            "Hidden_dim_0 -0.448621 -0.460639  1.057122  0.343618 -1.763040\n",
            "Hidden_dim_1  1.657340 -0.385082 -0.676922  0.611676  1.031000\n",
            "Hidden_dim_2  0.515000 -0.839218 -0.309212  0.331263  0.975545\n",
            "\n",
            "Final Output weight matrix of the hidden layer (W^T . h):\n",
            "        Hidden_dim_0  Hidden_dim_1  Hidden_dim_2\n",
            "eat        -0.188656     -0.895901     -1.324995\n",
            "apple      -1.034224      0.669129      1.224793\n",
            "banana     -0.175211      1.146773      0.440723\n",
            "sapota     -0.738841      0.637400      1.588048\n",
            "carrot     -0.291405      1.999038     -2.398737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "term = \"eat\"\n",
        "\n",
        "predicted_word, output_probs = cbow.predict(term)\n",
        "\n",
        "print(\"Predicted word after '{}': {}\".format(term, predicted_word))\n",
        "print(\"Softmax function output values (probabilities):\")\n",
        "for word, prob in zip(sorted_words, output_probs):\n",
        "    print(f\"{word}: {prob:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nS7mEtx4vKm",
        "outputId": "4b0cb3d3-046f-46d7-f997-f682ec2a072b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted word after 'eat': carrot\n",
            "Softmax function output values (probabilities):\n",
            "eat: 0.003419\n",
            "apple: 0.248574\n",
            "banana: 0.249184\n",
            "sapota: 0.249065\n",
            "carrot: 0.249758\n"
          ]
        }
      ]
    }
  ]
}